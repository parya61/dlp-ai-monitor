"""
scripts/generate_large_dataset.py

–ß–¢–û: –°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ DLP-–∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤ + –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î
–ó–ê–ß–ï–ú: Production-ready –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–∏

–ó–ê–ü–£–°–ö:
    python scripts/generate_large_dataset.py --n_incidents 30000

–ß–¢–û –°–û–ó–î–ê–Å–¢–°–Ø:
1. data/synthetic/incidents_30k.csv - –¥–∞—Ç–∞—Å–µ—Ç –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤
2. data/vector_db/incidents_30k.faiss - FAISS –∏–Ω–¥–µ–∫—Å
3. data/vector_db/incidents_30k_metadata.pkl - –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
"""

import argparse
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.config import get_config
from src.data import DataLoader
from src.data.augmentation import EnhancedDLPGenerator
from src.utils import get_logger
from src.vector_db import IncidentEmbedder, FAISSStore

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
logger = get_logger(__name__)
config = get_config()


def generate_dataset(
    n_incidents: int = 30000,
    output_name: str = None,
    seed: int = 42
):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç + –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î.
    
    Args:
        n_incidents: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤
        output_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ (default: incidents_{n}k)
        seed: Random seed
    """
    logger.info("=" * 80)
    logger.info(f"GENERATING LARGE DATASET: {n_incidents} incidents")
    logger.info("=" * 80)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–æ–≤
    if output_name is None:
        n_k = n_incidents // 1000
        output_name = f"incidents_{n_k}k"
    
    # –ü—É—Ç–∏
    csv_path = config.get_data_path(f"{output_name}.csv", subdir="synthetic")
    faiss_path = config.get_data_path(output_name, subdir="vector_db")
    
    # =========================================================================
    # –®–ê–ì 1: –ì–ï–ù–ï–†–ê–¶–ò–Ø –î–ê–¢–ê–°–ï–¢–ê
    # =========================================================================
    
    logger.info("\n" + "=" * 80)
    logger.info("STEP 1: Generating incidents")
    logger.info("=" * 80)
    
    generator = EnhancedDLPGenerator(seed=seed)
    df = generator.generate(n_incidents=n_incidents, show_progress=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV
    loader = DataLoader()
    loader.save_csv(df, csv_path)
    
    logger.info(f"\n‚úÖ Dataset saved: {csv_path}")
    logger.info(f"   Shape: {df.shape}")
    logger.info(f"   Size: {csv_path.stat().st_size / 1024 / 1024:.2f} MB")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info("\nüìä Dataset statistics:")
    logger.info(f"   Incident types: {df['incident_type'].value_counts().to_dict()}")
    logger.info(f"   Severity: {df['severity'].value_counts().to_dict()}")
    logger.info(f"   Departments: {df['department'].nunique()} unique")
    logger.info(f"   Users: {df['user'].nunique()} unique")
    
    # =========================================================================
    # –®–ê–ì 2: –°–û–ó–î–ê–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í
    # =========================================================================
    
    logger.info("\n" + "=" * 80)
    logger.info("STEP 2: Creating embeddings")
    logger.info("=" * 80)
    
    embedder = IncidentEmbedder()
    
    texts = df['description'].fillna("").tolist()
    vectors = embedder.encode(texts, batch_size=64, show_progress=True)
    
    logger.info(f"\n‚úÖ Embeddings created")
    logger.info(f"   Shape: {vectors.shape}")
    logger.info(f"   Dimension: {embedder.dimension}")
    logger.info(f"   Memory: {vectors.nbytes / 1024 / 1024:.2f} MB")
    
    # =========================================================================
    # –®–ê–ì 3: –°–û–ó–î–ê–ù–ò–ï FAISS –ò–ù–î–ï–ö–°–ê
    # =========================================================================
    
    logger.info("\n" + "=" * 80)
    logger.info("STEP 3: Building FAISS index")
    logger.info("=" * 80)
    
    # –°–æ–∑–¥–∞—ë–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = []
    for i, row in df.iterrows():
        meta = {
            "id": i,
            "index": i,
            **row.to_dict()
        }
        metadata.append(meta)
    
    # –°–æ–∑–¥–∞—ë–º store
    store = FAISSStore(dimension=embedder.dimension)
    store.add(vectors, metadata)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    store.save(faiss_path)
    
    logger.info(f"\n‚úÖ FAISS index saved: {faiss_path}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = store.get_stats()
    logger.info(f"\nüìä FAISS stats:")
    for key, value in stats.items():
        logger.info(f"   {key}: {value}")
    
    # =========================================================================
    # –®–ê–ì 4: –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û–ò–°–ö–ê
    # =========================================================================
    
    logger.info("\n" + "=" * 80)
    logger.info("STEP 4: Testing similarity search")
    logger.info("=" * 80)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    test_query = "–û—Ç–ø—Ä–∞–≤–∫–∞ email —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –Ω–∞ –ª–∏—á–Ω—É—é –ø–æ—á—Ç—É"
    logger.info(f"\nTest query: {test_query}")
    
    query_vector = embedder.encode_one(test_query)
    similar = store.search(query_vector, k=5)
    
    logger.info(f"\nTop 5 similar incidents:")
    for i, item in enumerate(similar):
        meta = item['metadata']
        logger.info(f"\n{i+1}. Similarity: {item['similarity']:.3f}")
        logger.info(f"   Type: {meta.get('incident_type')}, Severity: {meta.get('severity')}")
        logger.info(f"   {meta.get('description', '')[:100]}...")
    
    # =========================================================================
    # –§–ò–ù–ê–õ
    # =========================================================================
    
    logger.info("\n" + "=" * 80)
    logger.info("‚úÖ GENERATION COMPLETE!")
    logger.info("=" * 80)
    
    logger.info("\nüìÅ Created files:")
    logger.info(f"   1. {csv_path}")
    logger.info(f"   2. {faiss_path}.faiss")
    logger.info(f"   3. {faiss_path}_metadata.pkl")
    
    logger.info("\nüöÄ Next steps:")
    logger.info("   1. Train ML model: python -m src.ml.train")
    logger.info("   2. Explore data: jupyter notebook notebooks/01_data_exploration.ipynb")
    logger.info("   3. Test similarity: python -m src.vector_db.similarity")
    
    return df, store


def main():
    """Main —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è CLI."""
    parser = argparse.ArgumentParser(
        description="Generate large DLP incidents dataset with vector DB"
    )
    
    parser.add_argument(
        "--n_incidents",
        type=int,
        default=30000,
        help="Number of incidents to generate (default: 30000)"
    )
    
    parser.add_argument(
        "--output_name",
        type=str,
        default=None,
        help="Output file name (default: incidents_{n}k)"
    )
    
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed (default: 42)"
    )
    
    args = parser.parse_args()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
    generate_dataset(
        n_incidents=args.n_incidents,
        output_name=args.output_name,
        seed=args.seed
    )


if __name__ == "__main__":
    main()