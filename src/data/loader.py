"""
src/data/loader.py

–ß–¢–û: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
–ó–ê–ß–ï–ú: –ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å CSV, Parquet, Excel, JSON

–û–°–ù–û–í–ù–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
- –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
- –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞—Ç–∞—Å–µ—Ç–µ

–ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:
    from src.data import DataLoader
    
    loader = DataLoader()
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å CSV
    df = loader.load_csv("data/synthetic/incidents.csv")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç
    df = loader.load_auto("data/incidents.parquet")
    
    # –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    info = loader.get_data_info(df)
"""

from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd

from src.config import get_config
from src.utils import get_logger

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
logger = get_logger(__name__)
config = get_config()


class DataLoader:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:
    - CSV (.csv)
    - Parquet (.parquet)
    - Excel (.xlsx, .xls)
    - JSON (.json)
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö."""
        logger.info("Initialized DataLoader")
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        self.supported_formats = [".csv", ".parquet", ".xlsx", ".xls", ".json"]
    
    def load_csv(
        self,
        filepath: str | Path,
        encoding: str = "utf-8",
        **kwargs
    ) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV —Ñ–∞–π–ª–∞.
        
        Args:
            filepath: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
            encoding: –ö–æ–¥–∏—Ä–æ–≤–∫–∞ —Ñ–∞–π–ª–∞ (default: utf-8)
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è pd.read_csv()
        
        Returns:
            pd.DataFrame: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        Example:
            df = loader.load_csv("data/synthetic/incidents.csv")
        """
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"File not found: {filepath}")
        
        logger.info(f"Loading CSV from {filepath}...")
        
        try:
            df = pd.read_csv(filepath, encoding=encoding, **kwargs)
            logger.info(f"Loaded {len(df)} rows, {len(df.columns)} columns")
            return df
        
        except UnicodeDecodeError:
            # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É (utf-8-sig –¥–ª—è —Ñ–∞–π–ª–æ–≤ —Å BOM)
            logger.warning(f"Failed with {encoding}, trying utf-8-sig...")
            df = pd.read_csv(filepath, encoding="utf-8-sig", **kwargs)
            logger.info(f"Loaded {len(df)} rows, {len(df.columns)} columns")
            return df
        
        except Exception as e:
            logger.error(f"Failed to load CSV: {e}")
            raise
    
    def load_parquet(
        self,
        filepath: str | Path,
        **kwargs
    ) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Parquet —Ñ–∞–π–ª–∞.
        
        Parquet - —ç—Ç–æ –∫–æ–ª–æ–Ω–æ—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
        –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –±—ã—Å—Ç—Ä–µ–µ —á–µ–º CSV, –º–µ–Ω—å—à–µ –º–µ—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            filepath: –ü—É—Ç—å –∫ Parquet —Ñ–∞–π–ª—É
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è pd.read_parquet()
        
        Returns:
            pd.DataFrame: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        Example:
            df = loader.load_parquet("data/processed/incidents.parquet")
        """
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"File not found: {filepath}")
        
        logger.info(f"Loading Parquet from {filepath}...")
        
        try:
            df = pd.read_parquet(filepath, **kwargs)
            logger.info(f"Loaded {len(df)} rows, {len(df.columns)} columns")
            return df
        
        except Exception as e:
            logger.error(f"Failed to load Parquet: {e}")
            raise
    
    def load_excel(
        self,
        filepath: str | Path,
        sheet_name: str | int = 0,
        **kwargs
    ) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Excel —Ñ–∞–π–ª–∞.
        
        Args:
            filepath: –ü—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É (.xlsx –∏–ª–∏ .xls)
            sheet_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∏–ª–∏ –∏–Ω–¥–µ–∫—Å –ª–∏—Å—Ç–∞ (default: 0 - –ø–µ—Ä–≤—ã–π –ª–∏—Å—Ç)
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è pd.read_excel()
        
        Returns:
            pd.DataFrame: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        Example:
            df = loader.load_excel("data/incidents.xlsx", sheet_name="Sheet1")
        """
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"File not found: {filepath}")
        
        logger.info(f"Loading Excel from {filepath}, sheet: {sheet_name}...")
        
        try:
            df = pd.read_excel(filepath, sheet_name=sheet_name, **kwargs)
            logger.info(f"Loaded {len(df)} rows, {len(df.columns)} columns")
            return df
        
        except Exception as e:
            logger.error(f"Failed to load Excel: {e}")
            raise
    
    def load_json(
        self,
        filepath: str | Path,
        orient: str = "records",
        **kwargs
    ) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞.
        
        Args:
            filepath: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É
            orient: –§–æ—Ä–º–∞—Ç JSON ('records', 'split', 'index', 'columns', 'values')
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è pd.read_json()
        
        Returns:
            pd.DataFrame: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        Example:
            df = loader.load_json("data/incidents.json")
        """
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"File not found: {filepath}")
        
        logger.info(f"Loading JSON from {filepath}...")
        
        try:
            df = pd.read_json(filepath, orient=orient, **kwargs)
            logger.info(f"Loaded {len(df)} rows, {len(df.columns)} columns")
            return df
        
        except Exception as e:
            logger.error(f"Failed to load JSON: {e}")
            raise
    
    def load_auto(self, filepath: str | Path, **kwargs) -> pd.DataFrame:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ.
        
        –ó–ê–ß–ï–ú: –ù–µ –Ω—É–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å, –∫–∞–∫–æ–π –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞.
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –º–µ—Ç–æ–¥–∞
        
        Returns:
            pd.DataFrame: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        Example:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Ñ–æ—Ä–º–∞—Ç
            df = loader.load_auto("data/incidents.csv")
            df = loader.load_auto("data/incidents.parquet")
        """
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"File not found: {filepath}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
        suffix = filepath.suffix.lower()
        
        if suffix == ".csv":
            return self.load_csv(filepath, **kwargs)
        elif suffix == ".parquet":
            return self.load_parquet(filepath, **kwargs)
        elif suffix in [".xlsx", ".xls"]:
            return self.load_excel(filepath, **kwargs)
        elif suffix == ".json":
            return self.load_json(filepath, **kwargs)
        else:
            raise ValueError(
                f"Unsupported file format: {suffix}. "
                f"Supported formats: {self.supported_formats}"
            )
    
    @staticmethod
    def get_data_info(df: pd.DataFrame) -> Dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ.
        
        –ó–ê–ß–ï–ú: –ë—ã—Å—Ç—Ä–æ –ø–æ–Ω—è—Ç—å, —á—Ç–æ –∑–∞ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.
        
        Args:
            df: DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π:
                - n_rows: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
                - n_columns: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫
                - columns: —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
                - dtypes: —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
                - missing_values: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤
                - memory_usage: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        
        Example:
            info = loader.get_data_info(df)
            print(f"Rows: {info['n_rows']}, Columns: {info['n_columns']}")
        """
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
        missing = df.isnull().sum()
        missing_dict = missing[missing > 0].to_dict()
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        memory_bytes = df.memory_usage(deep=True).sum()
        memory_mb = memory_bytes / (1024 * 1024)
        
        info = {
            "n_rows": len(df),
            "n_columns": len(df.columns),
            "columns": df.columns.tolist(),
            "dtypes": df.dtypes.astype(str).to_dict(),
            "missing_values": missing_dict if missing_dict else "No missing values",
            "memory_usage_mb": round(memory_mb, 2),
        }
        
        return info
    
    @staticmethod
    def save_csv(
        df: pd.DataFrame,
        filepath: str | Path,
        encoding: str = "utf-8-sig",
        index: bool = False,
        **kwargs
    ) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ CSV —Ñ–∞–π–ª.
        
        Args:
            df: DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            encoding: –ö–æ–¥–∏—Ä–æ–≤–∫–∞ (default: utf-8-sig –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Excel)
            index: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –∏–Ω–¥–µ–∫—Å (default: False)
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è df.to_csv()
        
        Example:
            loader.save_csv(df, "data/output/result.csv")
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Saving CSV to {filepath}...")
        df.to_csv(filepath, encoding=encoding, index=index, **kwargs)
        logger.info(f"Saved {len(df)} rows to {filepath}")
    
    @staticmethod
    def save_parquet(
        df: pd.DataFrame,
        filepath: str | Path,
        **kwargs
    ) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ Parquet —Ñ–∞–π–ª.
        
        Args:
            df: DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è df.to_parquet()
        
        Example:
            loader.save_parquet(df, "data/processed/result.parquet")
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Saving Parquet to {filepath}...")
        df.to_parquet(filepath, **kwargs)
        logger.info(f"Saved {len(df)} rows to {filepath}")


# =============================================================================
# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
# =============================================================================

if __name__ == "__main__":
    # –°–æ–∑–¥–∞—ë–º –∑–∞–≥—Ä—É–∑—á–∏–∫
    loader = DataLoader()
    
    print("=" * 80)
    print("DataLoader - Example Usage")
    print("=" * 80)
    
    # –ü—Ä–∏–º–µ—Ä 1: –ó–∞–≥—Ä—É–∑–∫–∞ CSV (–µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
    csv_path = config.get_data_path("incidents_sample.csv", subdir="synthetic")
    
    if csv_path.exists():
        print(f"\nüìÅ Loading CSV from: {csv_path}")
        df = loader.load_csv(csv_path)
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
        info = loader.get_data_info(df)
        
        print(f"\nüìä Dataset Info:")
        print(f"  Rows: {info['n_rows']}")
        print(f"  Columns: {info['n_columns']}")
        print(f"  Memory: {info['memory_usage_mb']} MB")
        print(f"\n  Column names: {', '.join(info['columns'][:5])}...")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏
        print(f"\nüìã First 3 rows:")
        print(df.head(3).to_string())
    
    else:
        print(f"\n‚ö†Ô∏è  File not found: {csv_path}")
        print("Run 'python -m src.data.generator' first to generate sample data")
    
    print("\n" + "=" * 80)